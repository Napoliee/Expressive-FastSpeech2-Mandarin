#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESD (Emotional Speech Dataset) ä¸­æ–‡æ•°æ®é¢„å¤„ç†è„šæœ¬
åªå¤„ç†ä¸­æ–‡è¯´è¯äººï¼ˆ0001-0010ï¼‰ï¼Œæ­£ç¡®ä¿ç•™æƒ…ç»ªæ ‡ç­¾ä¿¡æ¯
"""

import os
import shutil
from pathlib import Path
import tqdm
import json
import random
from collections import defaultdict

def prepare_esd_chinese_data():
    """é¢„å¤„ç†ESDä¸­æ–‡æ•°æ®é›†ï¼Œä¿ç•™æƒ…ç»ªä¿¡æ¯"""
    
    # è·¯å¾„é…ç½®
    source_dir = "./Emotional Speech Dataset (ESD)/Emotion Speech Dataset"
    target_dir = "./raw_data/ESD-Chinese-Singing-MFA"
    
    print("ğŸ¯ å¼€å§‹å¤„ç†ESDä¸­æ–‡æ•°æ®é›†...")
    
    # æ¸…ç†å¹¶åˆ›å»ºç›®æ ‡ç›®å½•
    if os.path.exists(target_dir):
        shutil.rmtree(target_dir)
    os.makedirs(target_dir, exist_ok=True)
    
    # æƒ…ç»ªæ˜ å°„ï¼ˆä¸­æ–‡åˆ°è‹±æ–‡ï¼‰
    emotion_mapping = {
        "ä¸­ç«‹": "Neutral",
        "å¼€å¿ƒ": "Happy", 
        "ä¼¤å¿ƒ": "Sad",
        "æ„¤æ€’": "Angry",
        "æƒŠè®¶": "Surprise"
    }
    
    # è‹±æ–‡æƒ…ç»ªåˆ°IDçš„æ˜ å°„
    emotion_id_mapping = {
        "Neutral": 0,
        "Happy": 1,
        "Sad": 2,
        "Angry": 3,
        "Surprise": 4
    }
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_files = 0
    emotion_counts = {emotion: 0 for emotion in emotion_id_mapping.keys()}
    
    # å­˜å‚¨æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯ï¼ˆç”¨äºåˆ›å»ºfilelistï¼‰
    all_files_info = []
    
    # åªå¤„ç†ä¸­æ–‡è¯´è¯äººï¼ˆ0001-0010ï¼‰
    for speaker_id in range(1, 11):  # 0001-0010ä¸ºä¸­æ–‡
        speaker_folder = f"{speaker_id:04d}"
        source_speaker_dir = os.path.join(source_dir, speaker_folder)
        
        if not os.path.exists(source_speaker_dir):
            continue
            
        print(f"ğŸ“‚ å¤„ç†ä¸­æ–‡è¯´è¯äºº: {speaker_folder}")
        
        # è¯»å–æ–‡æœ¬æ–‡ä»¶
        text_file = os.path.join(source_speaker_dir, f"{speaker_folder}.txt")
        if not os.path.exists(text_file):
            print(f"âŒ æ‰¾ä¸åˆ°æ–‡æœ¬æ–‡ä»¶: {text_file}")
            continue
            
        # è§£ææ–‡æœ¬æ–‡ä»¶
        text_dict = {}
        with open(text_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    parts = line.strip().split('\t')
                    if len(parts) >= 3:
                        file_id, text, emotion_cn = parts[0], parts[1], parts[2]
                        emotion_en = emotion_mapping.get(emotion_cn, "Neutral")
                        text_dict[file_id] = {
                            'text': text,
                            'emotion_cn': emotion_cn,
                            'emotion_en': emotion_en,
                            'emotion_id': emotion_id_mapping[emotion_en]
                        }
        
        # åˆ›å»ºè¯´è¯äººç›®å½•
        target_speaker_dir = os.path.join(target_dir, speaker_folder)
        os.makedirs(target_speaker_dir, exist_ok=True)
        
        # å¤„ç†æ¯ä¸ªæƒ…ç»ªæ–‡ä»¶å¤¹
        for emotion_folder in ["Angry", "Happy", "Neutral", "Sad", "Surprise"]:
            source_emotion_dir = os.path.join(source_speaker_dir, emotion_folder)
            
            if not os.path.exists(source_emotion_dir):
                continue
                
            # å¤åˆ¶éŸ³é¢‘æ–‡ä»¶å¹¶åˆ›å»ºå¯¹åº”çš„æ–‡æœ¬æ–‡ä»¶
            wav_files = list(Path(source_emotion_dir).glob("*.wav"))
            
            for wav_file in tqdm.tqdm(wav_files, desc=f"  {emotion_folder}"):
                file_id = wav_file.stem  # ä¸åŒ…å«æ‰©å±•å
                
                if file_id in text_dict:
                    # å¤åˆ¶éŸ³é¢‘æ–‡ä»¶
                    target_wav = os.path.join(target_speaker_dir, f"{file_id}.wav")
                    shutil.copy2(wav_file, target_wav)
                    
                    # åˆ›å»ºå¯¹åº”çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆç”¨äºMFAï¼‰
                    target_lab = os.path.join(target_speaker_dir, f"{file_id}.lab")
                    with open(target_lab, 'w', encoding='utf-8') as f:
                        f.write(text_dict[file_id]['text'])
                    
                    # è®°å½•æ–‡ä»¶ä¿¡æ¯ï¼ˆä½¿ç”¨å®é™…çš„æƒ…ç»ªæ–‡ä»¶å¤¹åç§°ï¼‰
                    actual_emotion_id = emotion_id_mapping[emotion_folder]
                    all_files_info.append({
                        'file_id': file_id,
                        'speaker_id': speaker_folder,
                        'speaker_idx': speaker_id - 1,  # ä»0å¼€å§‹
                        'emotion_folder': emotion_folder,
                        'emotion_id': actual_emotion_id,
                        'text': text_dict[file_id]['text']
                    })
                    
                    total_files += 1
                    emotion_counts[emotion_folder] += 1
                else:
                    print(f"âš ï¸  æ‰¾ä¸åˆ°æ–‡æœ¬ä¿¡æ¯: {file_id}")
    
    # åˆ›å»ºè®­ç»ƒæ–‡ä»¶åˆ—è¡¨ï¼ˆä½¿ç”¨åˆ†å±‚é‡‡æ ·ï¼‰
    create_stratified_split(target_dir, all_files_info)
    
    # åˆ›å»ºè¯´è¯äººä¿¡æ¯æ–‡ä»¶
    create_speaker_info(target_dir, list(range(1, 11)))
    
    # åˆ›å»ºæƒ…ç»ªä¿¡æ¯æ–‡ä»¶
    create_emotion_info(target_dir, emotion_id_mapping)
    
    print(f"\nâœ… ä¸­æ–‡æ•°æ®å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"ğŸ‘¥ è¯´è¯äººæ•°: 10 (ä¸­æ–‡)")
    print(f"ğŸ“ˆ æƒ…ç»ªåˆ†å¸ƒ:")
    for emotion, count in emotion_counts.items():
        print(f"   {emotion}: {count}")

def create_stratified_split(data_dir, files_info, val_ratio=0.15, test_ratio=0.05):
    """åˆ›å»ºåˆ†å±‚é‡‡æ ·çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†"""
    print("ğŸ“ åˆ›å»ºåˆ†å±‚é‡‡æ ·çš„æ•°æ®åˆ’åˆ†...")
    
    # è®¾ç½®éšæœºç§å­ä¿è¯å¯é‡å¤æ€§
    random.seed(42)
    
    # æŒ‰è¯´è¯äººå’Œæƒ…ç»ªåˆ†ç»„
    speaker_emotion_groups = defaultdict(lambda: defaultdict(list))
    
    for file_info in files_info:
        speaker_idx = file_info['speaker_idx']
        emotion_id = file_info['emotion_id']
        speaker_emotion_groups[speaker_idx][emotion_id].append(file_info)
    
    train_files = []
    val_files = []
    test_files = []
    
    # ä¸ºæ¯ä¸ªè¯´è¯äººçš„æ¯ç§æƒ…ç»ªè¿›è¡Œåˆ†å±‚é‡‡æ ·
    for speaker_idx in speaker_emotion_groups:
        for emotion_id in speaker_emotion_groups[speaker_idx]:
            emotion_files = speaker_emotion_groups[speaker_idx][emotion_id]
            random.shuffle(emotion_files)  # éšæœºæ‰“ä¹±
            
            n_files = len(emotion_files)
            n_test = max(1, int(n_files * test_ratio))  # è‡³å°‘1ä¸ªæµ‹è¯•æ ·æœ¬
            n_val = max(1, int(n_files * val_ratio))    # è‡³å°‘1ä¸ªéªŒè¯æ ·æœ¬
            n_train = n_files - n_test - n_val
            
            # åˆ†é…æ–‡ä»¶
            test_files.extend(emotion_files[:n_test])
            val_files.extend(emotion_files[n_test:n_test + n_val])
            train_files.extend(emotion_files[n_test + n_val:])
    
    # åˆ›å»ºæ–‡ä»¶åˆ—è¡¨æ ¼å¼ï¼šfile_id|speaker_id|emotion_id|text
    def create_filelist(files):
        filelist = []
        for file_info in files:
            line = f"{file_info['file_id']}|{file_info['speaker_idx']}|{file_info['emotion_id']}|{file_info['text']}"
            filelist.append(line)
        return filelist
    
    train_list = create_filelist(train_files)
    val_list = create_filelist(val_files)
    test_list = create_filelist(test_files)
    
    # ä¿å­˜æ–‡ä»¶åˆ—è¡¨
    with open(os.path.join(data_dir, "train.txt"), 'w', encoding='utf-8') as f:
        f.write('\n'.join(train_list))
    
    with open(os.path.join(data_dir, "val.txt"), 'w', encoding='utf-8') as f:
        f.write('\n'.join(val_list))
    
    with open(os.path.join(data_dir, "test.txt"), 'w', encoding='utf-8') as f:
        f.write('\n'.join(test_list))
    
    # ä¿å­˜å®Œæ•´æ–‡ä»¶åˆ—è¡¨
    all_list = train_list + val_list + test_list
    with open(os.path.join(data_dir, "filelist.txt"), 'w', encoding='utf-8') as f:
        f.write('\n'.join(all_list))
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"   ğŸ“Š æ•°æ®åˆ’åˆ†ç»Ÿè®¡:")
    print(f"     è®­ç»ƒé›†: {len(train_list)} æ¡ç›® ({len(train_list)/len(all_list)*100:.1f}%)")
    print(f"     éªŒè¯é›†: {len(val_list)} æ¡ç›® ({len(val_list)/len(all_list)*100:.1f}%)")
    print(f"     æµ‹è¯•é›†: {len(test_list)} æ¡ç›® ({len(test_list)/len(all_list)*100:.1f}%)")
    
    # éªŒè¯æ¯ä¸ªé›†åˆä¸­è¯´è¯äººå’Œæƒ…ç»ªçš„åˆ†å¸ƒ
    validate_split_distribution(train_files, val_files, test_files)

def validate_split_distribution(train_files, val_files, test_files):
    """éªŒè¯æ•°æ®åˆ’åˆ†çš„åˆ†å¸ƒæ˜¯å¦åˆç†"""
    print("   ğŸ” éªŒè¯æ•°æ®åˆ†å¸ƒ:")
    
    def get_distribution(files, name):
        speaker_count = defaultdict(int)
        emotion_count = defaultdict(int)
        
        for file_info in files:
            speaker_count[file_info['speaker_idx']] += 1
            emotion_count[file_info['emotion_id']] += 1
        
        print(f"     {name}:")
        print(f"       è¯´è¯äººåˆ†å¸ƒ: {dict(speaker_count)}")
        print(f"       æƒ…ç»ªåˆ†å¸ƒ: {dict(emotion_count)}")
    
    get_distribution(train_files, "è®­ç»ƒé›†")
    get_distribution(val_files, "éªŒè¯é›†")
    get_distribution(test_files, "æµ‹è¯•é›†")

def create_speaker_info(data_dir, speaker_ids):
    """åˆ›å»ºè¯´è¯äººä¿¡æ¯æ–‡ä»¶"""
    print("ğŸ‘¥ åˆ›å»ºè¯´è¯äººä¿¡æ¯...")
    
    speakers = [f"{sid:04d}" for sid in speaker_ids]
    
    speaker_info = {
        'n_speakers': len(speakers),
        'speakers': {str(i): speaker for i, speaker in enumerate(speakers)}
    }
    
    with open(os.path.join(data_dir, "speakers.json"), 'w', encoding='utf-8') as f:
        json.dump(speaker_info, f, ensure_ascii=False, indent=2)
    
    print(f"   è®°å½•äº† {len(speakers)} ä¸ªä¸­æ–‡è¯´è¯äºº")

def create_emotion_info(data_dir, emotion_mapping):
    """åˆ›å»ºæƒ…ç»ªä¿¡æ¯æ–‡ä»¶"""
    print("ğŸ˜Š åˆ›å»ºæƒ…ç»ªä¿¡æ¯...")
    
    emotion_info = {
        'n_emotions': len(emotion_mapping),
        'emotions': {str(v): k for k, v in emotion_mapping.items()}
    }
    
    with open(os.path.join(data_dir, "emotions.json"), 'w', encoding='utf-8') as f:
        json.dump(emotion_info, f, ensure_ascii=False, indent=2)
    
    print(f"   è®°å½•äº† {len(emotion_mapping)} ç§æƒ…ç»ª")
    print("   æƒ…ç»ªæ˜ å°„:")
    for emotion_id, emotion_name in emotion_info['emotions'].items():
        print(f"     {emotion_id}: {emotion_name}")

if __name__ == "__main__":
    prepare_esd_chinese_data() 