#!/usr/bin/env python3
"""
ä½¿ç”¨MFAå¯¹é½çš„æ­£ç¡®æ¨ç†è„šæœ¬
è®­ç»ƒæ—¶æ€ä¹ˆå¤„ç†ï¼Œæ¨ç†æ—¶å°±æ€ä¹ˆå¤„ç†
"""

import argparse
import os
import tempfile
import shutil
import subprocess
import torch
import yaml
import numpy as np
import textgrid
from torch.utils.data import DataLoader

from utils.model import get_model, get_vocoder
from utils.tools import to_device, synth_samples
from text.ipa_processor import text_to_sequence_ipa

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def create_temp_file_for_mfa(text, temp_dir):
    """ä¸ºMFAåˆ›å»ºä¸´æ—¶æ–‡ä»¶"""
    
    # åˆ›å»ºä¸´æ—¶éŸ³é¢‘æ–‡ä»¶ - ä½¿ç”¨ä¸€ä¸ªçŸ­çš„é™éŸ³éŸ³é¢‘ä½œä¸ºå ä½ç¬¦
    wav_path = os.path.join(temp_dir, "temp.wav")
    # åˆ›å»ºä¸€ä¸ªçŸ­çš„é™éŸ³éŸ³é¢‘ï¼ˆ1ç§’ï¼Œ22050é‡‡æ ·ç‡ï¼‰
    import soundfile as sf
    import numpy as np
    silent_audio = np.zeros(22050, dtype=np.float32)
    sf.write(wav_path, silent_audio, 22050)
    
    # åˆ›å»ºæ–‡æœ¬æ–‡ä»¶
    lab_path = os.path.join(temp_dir, "temp.lab")
    with open(lab_path, 'w', encoding='utf-8') as f:
        f.write(text)
    
    return wav_path, lab_path

def run_mfa_alignment(text, temp_dir):
    """è¿è¡ŒMFAå¯¹é½è·å–éŸ³ç´ """
    
    print(f"æ­£åœ¨å¯¹æ–‡æœ¬è¿›è¡ŒMFAå¯¹é½: {text}")
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    wav_path, lab_path = create_temp_file_for_mfa(text, temp_dir)
    
    # MFAå¯¹é½å‘½ä»¤
    output_dir = os.path.join(temp_dir, "output")
    os.makedirs(output_dir, exist_ok=True)
    
    # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„MFAæ¨¡å‹å’Œè¯å…¸
    mfa_cmd = [
        "mfa", "align",
        temp_dir,
                    "mandarin_mfa",  # ä½¿ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡è¯å…¸
            "mandarin_mfa",  # ä½¿ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡å£°å­¦æ¨¡å‹
        output_dir,
        "--clean"
    ]
    
    try:
        print("è¿è¡ŒMFAå¯¹é½...")
        result = subprocess.run(mfa_cmd, capture_output=True, text=True, timeout=300)
        
        if result.returncode != 0:
            print(f"MFAå¯¹é½å¤±è´¥:")
            print(f"stderr: {result.stderr}")
            print(f"stdout: {result.stdout}")
            return None
        
        # è¯»å–ç”Ÿæˆçš„TextGridæ–‡ä»¶
        textgrid_path = os.path.join(output_dir, "temp.TextGrid")
        if os.path.exists(textgrid_path):
            return extract_phonemes_from_textgrid(textgrid_path)
        else:
            print("æœªç”ŸæˆTextGridæ–‡ä»¶")
            return None
            
    except subprocess.TimeoutExpired:
        print("MFAå¯¹é½è¶…æ—¶")
        return None
    except Exception as e:
        print(f"MFAå¯¹é½å¼‚å¸¸: {e}")
        return None

def extract_phonemes_from_textgrid(textgrid_path):
    """ä»TextGridæ–‡ä»¶æå–éŸ³ç´ åºåˆ—ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒçš„é€»è¾‘ï¼‰"""
    try:
        tg = textgrid.TextGrid.fromFile(textgrid_path)
        phone_tier = None
        
        # æŸ¥æ‰¾phoneså±‚
        for tier in tg.tiers:
            if tier.name.lower() in ['phones', 'phone']:
                phone_tier = tier
                break
        
        if phone_tier is None:
            print("TextGridä¸­æœªæ‰¾åˆ°phoneså±‚")
            return None
        
        phonemes = []
        durations = []
        
        # æå–éŸ³ç´ å’Œæ—¶é•¿ï¼ˆä¸trainingæ—¶å®Œå…¨ç›¸åŒçš„é€»è¾‘ï¼‰
        for interval in phone_tier:
            phone = interval.mark.strip()
            duration_frames = int((interval.maxTime - interval.minTime) * 22050 / 256)  # hop_length=256
            
            # åªä¿ç•™éç©ºéŸ³ç´ ï¼ˆä¸è®­ç»ƒé€»è¾‘ä¸€è‡´ï¼‰
            if phone and phone != '':
                phonemes.append(phone)
                durations.append(max(1, duration_frames))
        
        print(f"æå–åˆ°éŸ³ç´ : {phonemes}")
        print(f"éŸ³ç´ æ•°é‡: {len(phonemes)}")
        
        return phonemes, durations
        
    except Exception as e:
        print(f"TextGridè§£æå¤±è´¥: {e}")
        return None

def get_latest_checkpoint():
    """è·å–æœ€æ–°çš„checkpoint"""
    checkpoint_dir = "output/ckpt/ESD-Chinese"
    if not os.path.exists(checkpoint_dir):
        return None
    
    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth.tar')]
    if not checkpoints:
        return None
    
    # æå–æ­¥æ•°å¹¶æ’åº
    checkpoint_steps = []
    for cp in checkpoints:
        try:
            step = int(cp.split('_')[0])
            checkpoint_steps.append(step)
        except:
            continue
    
    if checkpoint_steps:
        return max(checkpoint_steps)
    return None

def synthesize(model, configs, vocoder, batchs, control_values):
    preprocess_config, model_config, train_config = configs
    pitch_control, energy_control, duration_control = control_values

    for batch in batchs:
        batch = to_device(batch, device)
        with torch.no_grad():
            # Forward
            output = model(
                *(batch[2:]),
                p_control=pitch_control,
                e_control=energy_control,
                d_control=duration_control
            )
            synth_samples(
                batch,
                output,
                vocoder,
                model_config,
                preprocess_config,
                train_config["path"]["result_path"],
            )

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--restore_step", type=int, default=None, help="ä½¿ç”¨ç‰¹å®šæ­¥æ•°çš„checkpointï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°çš„")
    parser.add_argument("--text", type=str, default="ä½ å¥½ä¸–ç•Œ", help="è¦åˆæˆçš„ä¸­æ–‡æ–‡æœ¬")
    parser.add_argument("--speaker_id", type=str, default="0001", help="è¯´è¯äººID")
    parser.add_argument("--emotion", type=str, default="ä¸­ç«‹", help="æƒ…æ„Ÿ: å¼€å¿ƒ/ä¼¤å¿ƒ/æƒŠè®¶/æ„¤æ€’/ä¸­ç«‹")
    parser.add_argument("--pitch_control", type=float, default=1.0)
    parser.add_argument("--energy_control", type=float, default=1.0)
    parser.add_argument("--duration_control", type=float, default=1.0)
    args = parser.parse_args()

    # è·å–æœ€æ–°checkpoint
    if args.restore_step is None:
        latest_step = get_latest_checkpoint()
        if latest_step is None:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•checkpointæ–‡ä»¶")
            return
        args.restore_step = latest_step
        print(f"ğŸ”„ ä½¿ç”¨æœ€æ–°checkpoint: {args.restore_step}")
    
    # Read Config
    preprocess_config = yaml.load(open("config/ESD-Chinese/preprocess.yaml", "r"), Loader=yaml.FullLoader)
    model_config = yaml.load(open("config/ESD-Chinese/model.yaml", "r"), Loader=yaml.FullLoader)
    train_config = yaml.load(open("config/ESD-Chinese/train.yaml", "r"), Loader=yaml.FullLoader)
    configs = (preprocess_config, model_config, train_config)

    print(f"ğŸ¤ ä½¿ç”¨MFAå¯¹é½è¿›è¡Œæ­£ç¡®æ¨ç†")
    print(f"æ£€æŸ¥ç‚¹: {args.restore_step}")
    print(f"è¾“å…¥æ–‡æœ¬: {args.text}")
    print(f"è¯´è¯äºº: {args.speaker_id}")
    print(f"æƒ…æ„Ÿ: {args.emotion}")

    # åˆ›å»ºä¸´æ—¶ç›®å½•è¿›è¡ŒMFAå¯¹é½
    with tempfile.TemporaryDirectory() as temp_dir:
        print(f"ä¸´æ—¶ç›®å½•: {temp_dir}")
        
        # è¿è¡ŒMFAå¯¹é½
        alignment_result = run_mfa_alignment(args.text, temp_dir)
        
        if alignment_result is None:
            print("âŒ MFAå¯¹é½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
            # å¤‡ç”¨ï¼šä½¿ç”¨ä¸€ä¸ªå·²çŸ¥çš„éŸ³ç´ åºåˆ—
            print("âš ï¸  ä½¿ç”¨é»˜è®¤éŸ³ç´ åºåˆ—")
            phonemes = ["n", "iË¨Ë©Ë¦", "x", "awË¨Ë©Ë¦"]  # "ä½ å¥½"çš„éŸ³ç´ 
        else:
            phonemes, durations = alignment_result
    
    # è½¬æ¢ä¸ºIPAæ ¼å¼
    ipa_text = "{" + " ".join(phonemes) + "}"
    print(f"IPAéŸ³ç´ : {ipa_text}")
    
    # Get model
    model = get_model(args, configs, device, train=False)

    # Load vocoder
    vocoder = get_vocoder(model_config, device)

    # å‡†å¤‡è¾“å…¥æ•°æ®
    ids = [args.speaker_id + "_mfa_test"]
    raw_texts = [args.text]
    speakers = np.array([int(args.speaker_id)], dtype=np.int64)
    
    # æƒ…æ„Ÿæ˜ å°„
    emotion_map = {"å¼€å¿ƒ": 0, "ä¼¤å¿ƒ": 1, "æƒŠè®¶": 2, "æ„¤æ€’": 3, "ä¸­ç«‹": 4}
    emotions = np.array([emotion_map.get(args.emotion, 4)], dtype=np.int64)
    arousals = np.array([0.5], dtype=np.float32)
    valences = np.array([0.5], dtype=np.float32)
    
    # è½¬æ¢ä¸ºåºåˆ—
    try:
        text_sequence = np.array(text_to_sequence_ipa(ipa_text), dtype=np.int64)
        text_lens = np.array([len(text_sequence)], dtype=np.int64)
        
        print(f"éŸ³ç´ åºåˆ—é•¿åº¦: {text_lens[0]}")
        print(f"éŸ³ç´ IDåºåˆ—: {text_sequence}")
        
        # å¯¹textsè¿›è¡Œpaddingå¤„ç†ï¼ˆæ‰‹åŠ¨paddingï¼Œå› ä¸ºæˆ‘ä»¬åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼‰
        from utils.tools import pad_1D
        texts = pad_1D([text_sequence])
        
        batchs = [(ids, raw_texts, speakers, emotions, arousals, valences, texts, text_lens, max(text_lens))]

        control_values = args.pitch_control, args.energy_control, args.duration_control

        print(f"ğŸµ å¼€å§‹åˆæˆ...")
        synthesize(model, configs, vocoder, batchs, control_values)
        print(f"âœ… åˆæˆå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {train_config['path']['result_path']}")
        print(f"ğŸ§ éŸ³é¢‘æ–‡ä»¶: {args.speaker_id}_mfa_test.wav")
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 