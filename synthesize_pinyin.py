 #!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import json
import numpy as np
import tempfile
import subprocess
import shutil
from pypinyin import lazy_pinyin, Style
import textgrid

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
import yaml
from model import FastSpeech2, ScheduledOptim
from utils.model import get_model, get_vocoder
from utils.tools import to_device, synth_samples
from text import text_to_sequence

class PinyinBasedInference:
    """åŸºäºæ‹¼éŸ³çš„ä¸­æ–‡TTSæ¨ç†å™¨"""
    
    def __init__(self, configs):
        self.preprocess_config, self.model_config, self.train_config = configs
        
        print("ğŸš€ åˆå§‹åŒ–åŸºäºæ‹¼éŸ³çš„ä¸­æ–‡TTSæ¨ç†å™¨")
        print("âœ… é…ç½®åŠ è½½å®Œæˆ")
    
    def chinese_to_pinyin(self, text):
        """å°†ä¸­æ–‡è½¬æ¢ä¸ºæ‹¼éŸ³ï¼ˆå¸¦å£°è°ƒæ•°å­—ï¼‰"""
        print(f"ğŸ”¤ ä¸­æ–‡è½¬æ‹¼éŸ³: {text}")
        
        # ä½¿ç”¨pypinyinè½¬æ¢ä¸ºå¸¦å£°è°ƒæ•°å­—çš„æ‹¼éŸ³
        pinyin_list = lazy_pinyin(text, style=Style.TONE3, neutral_tone_with_five=True)
        
        # è¿‡æ»¤æ‰æ ‡ç‚¹ç¬¦å·
        filtered_pinyin = []
        for py in pinyin_list:
            # åªä¿ç•™å­—æ¯å’Œæ•°å­—ï¼ˆæ‹¼éŸ³æ ¼å¼ï¼‰
            if py.isalnum() or any(char.isalpha() for char in py):
                filtered_pinyin.append(py)
        
        pinyin_text = " ".join(filtered_pinyin)
        print(f"ğŸ“ æ‹¼éŸ³ç»“æœ: {pinyin_text}")
        
        return pinyin_text
    
    def create_temp_files_for_mfa(self, pinyin_text, temp_dir):
        """ä¸ºMFAåˆ›å»ºä¸´æ—¶æ–‡ä»¶"""
        
        # åˆ›å»ºä¸´æ—¶éŸ³é¢‘æ–‡ä»¶ï¼ˆMFAéœ€è¦ï¼Œä½†æˆ‘ä»¬åªéœ€è¦æ–‡æœ¬å¯¹é½ï¼‰
        wav_path = os.path.join(temp_dir, "temp.wav")
        
        # åˆ›å»ºä¸€ä¸ª1ç§’çš„é™éŸ³éŸ³é¢‘ï¼ˆMFAéœ€è¦éŸ³é¢‘æ–‡ä»¶ï¼‰
        import numpy as np
        from scipy.io import wavfile
        sample_rate = 22050
        duration = max(1.0, len(pinyin_text.split()) * 0.2)  # æ ¹æ®æ‹¼éŸ³æ•°é‡ä¼°ç®—æ—¶é•¿
        samples = np.zeros(int(sample_rate * duration), dtype=np.float32)
        wavfile.write(wav_path, sample_rate, samples)
        
        # åˆ›å»ºæ–‡æœ¬æ–‡ä»¶
        lab_path = os.path.join(temp_dir, "temp.lab")
        with open(lab_path, 'w', encoding='utf-8') as f:
            f.write(pinyin_text)
        
        print(f"âœ… åˆ›å»ºä¸´æ—¶æ–‡ä»¶:")
        print(f"   éŸ³é¢‘: {wav_path}")
        print(f"   æ–‡æœ¬: {lab_path}")
        
        return wav_path, lab_path
    
    def run_mfa_alignment(self, pinyin_text, temp_dir):
        """è¿è¡ŒMFAå¯¹é½è·å–æ‹¼éŸ³éŸ³ç´ """
        
        print(f"ğŸ”§ å‡†å¤‡MFAå¯¹é½...")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        wav_path, lab_path = self.create_temp_files_for_mfa(pinyin_text, temp_dir)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.join(temp_dir, "output")
        os.makedirs(output_dir, exist_ok=True)
        
        # MFAå‘½ä»¤ï¼ˆä½¿ç”¨ä¸­æ–‡æ‹¼éŸ³æ¨¡å‹ï¼‰
        mfa_cmd = [
            "mfa", "align",
            temp_dir,
            "mandarin_pinyin",  # ä½¿ç”¨ä¸­æ–‡æ‹¼éŸ³è¯å…¸
            "mandarin_mfa",     # ä½¿ç”¨ä¸­æ–‡å£°å­¦æ¨¡å‹
            output_dir,
            "--clean"
        ]
        
        try:
            print("ğŸ”„ è¿è¡ŒMFAå¯¹é½...")
            
            # åœ¨alignerç¯å¢ƒä¸­è¿è¡ŒMFA
            conda_cmd = [
                "conda", "run", "-n", "aligner"
            ] + mfa_cmd
            
            print(f"   å‘½ä»¤: {' '.join(conda_cmd)}")
            
            result = subprocess.run(conda_cmd, capture_output=True, text=True, timeout=120)
            
            if result.returncode != 0:
                print(f"âŒ MFAå¯¹é½å¤±è´¥:")
                print(f"   stderr: {result.stderr}")
                return None
            
            # æ£€æŸ¥ç”Ÿæˆçš„TextGridæ–‡ä»¶
            textgrid_path = os.path.join(output_dir, "temp.TextGrid")
            if os.path.exists(textgrid_path):
                print("âœ… MFAå¯¹é½æˆåŠŸï¼Œæå–éŸ³ç´ ...")
                return self.extract_phonemes_from_textgrid(textgrid_path)
            else:
                print("âŒ æœªç”ŸæˆTextGridæ–‡ä»¶")
                return None
                
        except subprocess.TimeoutExpired:
            print("âŒ MFAå¯¹é½è¶…æ—¶")
            return None
        except Exception as e:
            print(f"âŒ MFAå¯¹é½å¼‚å¸¸: {e}")
            return None
    
    def extract_phonemes_from_textgrid(self, textgrid_path):
        """ä»TextGridæå–éŸ³ç´ """
        
        try:
            tg = textgrid.TextGrid.fromFile(textgrid_path)
            
            # æŸ¥æ‰¾phoneså±‚
            phone_tier = None
            for tier in tg.tiers:
                if tier.name.lower() in ['phones', 'phone']:
                    phone_tier = tier
                    break
            
            if phone_tier is None:
                print("âŒ TextGridä¸­æœªæ‰¾åˆ°phoneså±‚")
                return None
            
            # æå–éŸ³ç´ 
            sil_phones = ["sil", "sp", "spn", ""]
            phones = []
            
            for interval in phone_tier:
                phone = interval.mark.strip()
                if phone and phone not in sil_phones:
                    phones.append(phone)
            
            print(f"âœ… æå–åˆ°éŸ³ç´ : {phones}")
            print(f"   éŸ³ç´ æ•°é‡: {len(phones)}")
            
            return phones
            
        except Exception as e:
            print(f"âŒ æå–éŸ³ç´ å¤±è´¥: {e}")
            return None
    
    def phonemes_to_id_sequence(self, phonemes):
        """å°†éŸ³ç´ è½¬æ¢ä¸ºIDåºåˆ—"""
        
        # æ„é€ IPAæ ¼å¼å­—ç¬¦ä¸²
        ipa_text = "{" + " ".join(phonemes) + "}"
        print(f"ğŸ”¢ IPAéŸ³ç´ å­—ç¬¦ä¸²: {ipa_text}")
        
        try:
            # ä½¿ç”¨IPAæ¸…ç†å™¨
            sequence = text_to_sequence(ipa_text, ["basic_cleaners"])
            print(f"âœ… IDåºåˆ—: {sequence}")
            print(f"   åºåˆ—é•¿åº¦: {len(sequence)}")
            return np.array(sequence, dtype=np.int64)
        except Exception as e:
            print(f"âŒ éŸ³ç´ è½¬IDå¤±è´¥: {e}")
            # å°è¯•å¤‡ç”¨æ–¹æ¡ˆï¼šä»è®­ç»ƒæ•°æ®ä¸­æŸ¥æ‰¾ç›¸ä¼¼æ‹¼éŸ³
            print("ğŸ”„ å°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
            return self.fallback_pinyin_to_ids(phonemes)
    
    def fallback_pinyin_to_ids(self, phonemes):
        """å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•çš„æ‹¼éŸ³åˆ°IDæ˜ å°„"""
        
        # è¿™é‡Œå¯ä»¥å®ç°ä¸€ä¸ªç®€å•çš„æ‹¼éŸ³éŸ³ç´ æ˜ å°„
        # æˆ–è€…ä»è®­ç»ƒæ•°æ®ä¸­æŸ¥æ‰¾ç›¸ä¼¼çš„æ‹¼éŸ³åºåˆ—
        print("âš ï¸  ä½¿ç”¨å¤‡ç”¨éŸ³ç´ æ˜ å°„æ–¹æ¡ˆ")
        
        # ç®€å•åœ°å°†æ¯ä¸ªéŸ³ç´ æ˜ å°„ä¸ºä¸€ä¸ªé»˜è®¤ID
        # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„æ˜ å°„é€»è¾‘
        default_ids = [1] * len(phonemes)  # ä½¿ç”¨ID=1ä½œä¸ºé»˜è®¤
        return np.array(default_ids, dtype=np.int64)
    
    def synthesize_from_chinese(self, text, speaker_id, emotion_id):
        """ä»ä¸­æ–‡æ–‡æœ¬åˆæˆè¯­éŸ³"""
        
        print(f"ğŸ¤ åŸºäºæ‹¼éŸ³+MFAçš„ä¸­æ–‡TTSæ¨ç†")
        print(f"æ–‡æœ¬: {text}")
        print(f"è¯´è¯äºº: {speaker_id}")
        print(f"æƒ…æ„Ÿ: {emotion_id}")
        print("=" * 50)
        
        # æ­¥éª¤1ï¼šä¸­æ–‡è½¬æ‹¼éŸ³
        pinyin_text = self.chinese_to_pinyin(text)
        if not pinyin_text:
            print("âŒ ä¸­æ–‡è½¬æ‹¼éŸ³å¤±è´¥")
            return None
        
        # æ­¥éª¤2ï¼šç”¨æ‹¼éŸ³è¿›è¡ŒMFAå¯¹é½
        with tempfile.TemporaryDirectory() as temp_dir:
            phonemes = self.run_mfa_alignment(pinyin_text, temp_dir)
            
        if phonemes is None:
            print("âŒ æ— æ³•è·å–éŸ³ç´ åºåˆ—")
            return None
        
        # æ­¥éª¤3ï¼šè½¬æ¢ä¸ºIDåºåˆ—
        id_sequence = self.phonemes_to_id_sequence(phonemes)
        if id_sequence is None:
            print("âŒ æ— æ³•è½¬æ¢ä¸ºIDåºåˆ—")
            return None
        
        # æ­¥éª¤4ï¼šå‡†å¤‡æ¨¡å‹è¾“å…¥
        return self.prepare_model_input(text, speaker_id, emotion_id, id_sequence)
    
    def prepare_model_input(self, text, speaker_id, emotion_id, id_sequence):
        """å‡†å¤‡æ¨¡å‹è¾“å…¥"""
        
        # åŠ è½½æ˜ å°„
        with open(os.path.join(self.preprocess_config["path"]["preprocessed_path"], "speakers.json")) as f:
            speaker_map = json.load(f)
        
        with open(os.path.join(self.preprocess_config["path"]["preprocessed_path"], "emotions.json")) as f:
            emotion_data = json.load(f)
            emotion_map = emotion_data["emotion_dict"]
            arousal_map = emotion_data["arousal_dict"]
            valence_map = emotion_data["valence_dict"]
        
        # å‡†å¤‡æ•°æ®
        ids = [f"{speaker_id}_pinyin"]
        raw_texts = [text]
        speakers = np.array([speaker_map[speaker_id]], dtype=np.int64)
        emotions = np.array([emotion_map[emotion_id]], dtype=np.int64)
        arousals = np.array([arousal_map[emotion_id]], dtype=np.float32)
        valences = np.array([valence_map[emotion_id]], dtype=np.float32)
        
        text_lens = np.array([len(id_sequence)], dtype=np.int64)
        
        # Padding
        from utils.tools import pad_1D
        texts = pad_1D([id_sequence])
        
        batch = (ids, raw_texts, speakers, emotions, arousals, valences, texts, text_lens, max(text_lens))
        
        return batch

def synthesize(model, configs, vocoder, batchs, control_values):
    """åˆæˆå‡½æ•°"""
    preprocess_config, model_config, train_config = configs
    pitch_control, energy_control, duration_control = control_values

    for batch in batchs:
        batch = to_device(batch, device)
        with torch.no_grad():
            # Forward
            output = model(*(batch[2:]), p_control=pitch_control, e_control=energy_control, d_control=duration_control)
            
            synth_samples(
                batch,
                output,
                vocoder,
                model_config,
                preprocess_config,
                train_config["path"]["result_path"],
            )

def get_latest_checkpoint():
    """è·å–æœ€æ–°çš„æ£€æŸ¥ç‚¹"""
    ckpt_dir = "./output/ckpt/ESD-Chinese"
    if not os.path.exists(ckpt_dir):
        print(f"æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: {ckpt_dir}")
        return None
    
    ckpt_files = [f for f in os.listdir(ckpt_dir) if f.endswith('.pth.tar')]
    if not ckpt_files:
        print("æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶")
        return None
    
    # æŒ‰æ­¥æ•°æ’åºï¼Œå–æœ€æ–°çš„
    ckpt_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))
    latest_ckpt = os.path.join(ckpt_dir, ckpt_files[-1])
    print(f"ä½¿ç”¨æ£€æŸ¥ç‚¹: {latest_ckpt}")
    return latest_ckpt

def main():
    print("ğŸš€ å¯åŠ¨åŸºäºæ‹¼éŸ³çš„ä¸­æ–‡TTSæ¨ç†å™¨")
    
    # åŠ è½½é…ç½®
    preprocess_config = yaml.load(open("./config/ESD-Chinese/preprocess.yaml", "r"), Loader=yaml.FullLoader)
    model_config = yaml.load(open("./config/ESD-Chinese/model.yaml", "r"), Loader=yaml.FullLoader)
    train_config = yaml.load(open("./config/ESD-Chinese/train.yaml", "r"), Loader=yaml.FullLoader)
    configs = (preprocess_config, model_config, train_config)
    
    # è®¾ç½®è®¾å¤‡
    global device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    checkpoint_path = get_latest_checkpoint()
    if checkpoint_path is None:
        print("âŒ æ— æ³•æ‰¾åˆ°æ¨¡å‹æ£€æŸ¥ç‚¹")
        return
    
    # åˆ›å»ºargså¯¹è±¡
    import argparse
    args = argparse.Namespace()
    args.restore_step = int(os.path.basename(checkpoint_path).split('.')[0])
    
    model = get_model(args, configs, device, train=False)
    model.load_state_dict(torch.load(checkpoint_path, map_location=device, weights_only=False)["model"])
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½å£°ç å™¨
    vocoder = get_vocoder(model_config, device)
    print("âœ… å£°ç å™¨åŠ è½½å®Œæˆ")
    
    # åˆ›å»ºæ¨ç†å™¨
    inference = PinyinBasedInference(configs)
    
    # æµ‹è¯•æ¨ç†
    test_cases = [
        ("ä»–å¯¹è°éƒ½é‚£ä¹ˆå‹å¥½ã€‚", "0008", "æƒŠè®¶"),
        ("ä»Šå¤©å¤©æ°”çœŸä¸é”™ã€‚", "0009", "å¼€å¿ƒ"),
        ("æˆ‘å¾ˆé«˜å…´è§åˆ°ä½ ã€‚", "0010", "å¼€å¿ƒ"),
    ]
    
    print("\n" + "="*50)
    print("å¼€å§‹æµ‹è¯•æ¨ç†...")
    print("="*50)
    
    for text, speaker_id, emotion in test_cases:
        print(f"\nğŸ¯ æµ‹è¯•ç”¨ä¾‹:")
        print(f"   æ–‡æœ¬: {text}")
        print(f"   è¯´è¯äºº: {speaker_id}")
        print(f"   æƒ…æ„Ÿ: {emotion}")
        
        # åˆæˆ
        batch = inference.synthesize_from_chinese(text, speaker_id, emotion)
        if batch is not None:
            print("ğŸµ å¼€å§‹åˆæˆ...")
            control_values = (1.0, 1.0, 1.0)  # pitch, energy, duration
            try:
                synthesize(model, configs, vocoder, [batch], control_values)
                print("âœ… åˆæˆå®Œæˆï¼")
            except Exception as e:
                print(f"âŒ åˆæˆå¤±è´¥: {e}")
        else:
            print("âŒ æ— æ³•åˆ›å»ºè¾“å…¥æ‰¹æ¬¡")
        
        print("-" * 30)

if __name__ == "__main__":
    main()