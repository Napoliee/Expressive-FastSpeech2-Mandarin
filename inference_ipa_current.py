#!/usr/bin/env python3
"""
åŸºäºå½“å‰IPAè®­ç»ƒæ¨¡å‹çš„æ¨ç†è„šæœ¬
"""

import argparse
import os
import torch
import yaml
import numpy as np
from torch.utils.data import DataLoader

from utils.model import get_model, get_vocoder
from utils.tools import to_device, synth_samples
from dataset_ipa_fixed import TextDataset
from text.ipa_processor import text_to_sequence_ipa

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def preprocess_chinese_ipa(text, speaker="0001"):
    """
    ä½¿ç”¨IPAéŸ³ç´ é¢„å¤„ç†ä¸­æ–‡æ–‡æœ¬
    """
    
    # ç¤ºä¾‹ï¼šç®€å•çš„ä¸­æ–‡è¯å¯¹åº”IPAéŸ³ç´ æ˜ å°„
    chinese_to_ipa = {
        "ä½ å¥½": ["n", "iË¨Ë©Ë¦", "x", "awË¨Ë©Ë¦"],
        "ä¸–ç•Œ": ["Ê‚", "ÊÌ©Ë¥Ë©", "tÉ•", "j", "eË¥Ë©"],
        "ç¾ä¸½": ["m", "ejË¨Ë©Ë¦", "l", "iË¥Ë©"],
        "ä¸­å›½": ["ÊˆÊ‚", "oÅ‹Ë¥Ë©", "k", "uÌ¯oË¥Ë©"],
        "ä»Šå¤©": ["tÉ•", "inË¥Ë©", "tÊ°", "j", "anË¥Ë©"],
        "æ˜å¤©": ["m", "iÅ‹Ë¥Ë©", "tÊ°", "j", "anË¥Ë©"],
        "è°¢è°¢": ["É•", "j", "eË¥Ë©", "É•", "j", "eË¥Ë©"],
        "å¼€å¿ƒ": ["kÊ°", "ajË¥Ë©", "É•", "inË¥Ë©"],
        "ç”Ÿæ—¥å¿«ä¹": ["Ê‚", "É™Å‹Ë¥Ë©", "ÊÌ©Ë¥Ë©", "kÊ°", "uajË¥Ë©", "l", "É™Ë¥Ë©"],
        "æµ‹è¯•": ["tÊ°s", "É™Ë¥Ë©", "Ê‚", "ÊÌ©Ë¥Ë©"],
        "è¯­éŸ³": ["yË¨Ë©Ë¦", "inË¥Ë©"],
        "åˆæˆ": ["x", "É™Ë§Ë¥", "ÊˆÊ‚Ê°", "É™Å‹Ë§Ë¥"],
    }
    
    # å¦‚æœæ‰¾åˆ°æ˜ å°„å°±ä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤éŸ³ç´ 
    if text in chinese_to_ipa:
        ipa_phones = chinese_to_ipa[text]
    else:
        # é»˜è®¤éŸ³ç´ åºåˆ—
        ipa_phones = ["n", "iË¨Ë©Ë¦", "x", "awË¨Ë©Ë¦"]  # é»˜è®¤"ä½ å¥½"
    
    # è½¬æ¢ä¸ºIPAæ ¼å¼å­—ç¬¦ä¸²
    ipa_text = "{" + " ".join(ipa_phones) + "}"
    
    return ipa_text

def synthesize(model, configs, vocoder, batchs, control_values):
    preprocess_config, model_config, train_config = configs
    pitch_control, energy_control, duration_control = control_values

    for batch in batchs:
        batch = to_device(batch, device)
        with torch.no_grad():
            # Forward
            output = model(
                *(batch[2:]),
                p_control=pitch_control,
                e_control=energy_control,
                d_control=duration_control
            )
            synth_samples(
                batch,
                output,
                vocoder,
                model_config,
                preprocess_config,
                train_config["path"]["result_path"],
            )

def get_latest_checkpoint():
    """è·å–æœ€æ–°çš„checkpointæ­¥æ•°"""
    ckpt_dir = "./output/ckpt/ESD-Chinese"
    if not os.path.exists(ckpt_dir):
        return None
    
    ckpt_files = [f for f in os.listdir(ckpt_dir) if f.endswith('.pth.tar')]
    if not ckpt_files:
        return None
    
    # æå–æ­¥æ•°å¹¶æ‰¾åˆ°æœ€å¤§å€¼
    steps = []
    for f in ckpt_files:
        try:
            step = int(f.replace('.pth.tar', ''))
            steps.append(step)
        except:
            continue
    
    return max(steps) if steps else None

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--restore_step", type=int, default=None, help="ä½¿ç”¨ç‰¹å®šæ­¥æ•°çš„checkpointï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°çš„")
    parser.add_argument("--text", type=str, default="ä½ å¥½", help="æµ‹è¯•æ–‡æœ¬")
    parser.add_argument("--speaker_id", type=str, default="0001", help="speaker ID")
    parser.add_argument("--emotion", type=str, default="å¼€å¿ƒ", help="emotion: å¼€å¿ƒ/ä¼¤å¿ƒ/æƒŠè®¶/æ„¤æ€’/ä¸­ç«‹")
    parser.add_argument("--pitch_control", type=float, default=1.0)
    parser.add_argument("--energy_control", type=float, default=1.0)
    parser.add_argument("--duration_control", type=float, default=1.0)
    args = parser.parse_args()

    # è·å–æœ€æ–°checkpoint
    if args.restore_step is None:
        latest_step = get_latest_checkpoint()
        if latest_step is None:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•checkpointæ–‡ä»¶")
            return
        args.restore_step = latest_step
        print(f"ğŸ”„ ä½¿ç”¨æœ€æ–°checkpoint: {args.restore_step}")
    
    # Read Config
    preprocess_config = yaml.load(open("config/ESD-Chinese/preprocess.yaml", "r"), Loader=yaml.FullLoader)
    model_config = yaml.load(open("config/ESD-Chinese/model.yaml", "r"), Loader=yaml.FullLoader)
    train_config = yaml.load(open("config/ESD-Chinese/train.yaml", "r"), Loader=yaml.FullLoader)
    configs = (preprocess_config, model_config, train_config)

    print(f"ğŸ¤ ä½¿ç”¨IPAæ¨¡å‹æ¨ç†")
    print(f"æ£€æŸ¥ç‚¹: {args.restore_step}")
    print(f"è¾“å…¥æ–‡æœ¬: {args.text}")
    print(f"è¯´è¯äºº: {args.speaker_id}")
    print(f"æƒ…æ„Ÿ: {args.emotion}")

    # Get model
    model = get_model(args, configs, device, train=False)

    # Load vocoder
    vocoder = get_vocoder(model_config, device)

    # å‡†å¤‡è¾“å…¥æ•°æ®
    ids = [args.speaker_id + "_ipa_test"]
    raw_texts = [args.text]
    speakers = np.array([int(args.speaker_id)], dtype=np.int64)
    
    # æƒ…æ„Ÿæ˜ å°„
    emotion_map = {"å¼€å¿ƒ": 0, "ä¼¤å¿ƒ": 1, "æƒŠè®¶": 2, "æ„¤æ€’": 3, "ä¸­ç«‹": 4}
    emotions = np.array([emotion_map.get(args.emotion, 0)], dtype=np.int64)
    arousals = np.array([0.5], dtype=np.float32)
    valences = np.array([0.5], dtype=np.float32)
    
    # è½¬æ¢ä¸ºIPAéŸ³ç´ 
    try:
        ipa_text = preprocess_chinese_ipa(args.text, args.speaker_id)
        print(f"IPAéŸ³ç´ : {ipa_text}")
        
        # è½¬æ¢ä¸ºåºåˆ—
        text_sequence = np.array(text_to_sequence_ipa(ipa_text), dtype=np.int64)
        text_lens = np.array([len(text_sequence)], dtype=np.int64)
        
        print(f"éŸ³ç´ åºåˆ—é•¿åº¦: {text_lens[0]}")
        print(f"éŸ³ç´ IDåºåˆ—: {text_sequence}")
        
        # å¯¹textsè¿›è¡Œpaddingå¤„ç†ï¼ˆæ‰‹åŠ¨paddingï¼Œå› ä¸ºæˆ‘ä»¬åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼‰
        from utils.tools import pad_1D
        texts = pad_1D([text_sequence])
        
        batchs = [(ids, raw_texts, speakers, emotions, arousals, valences, texts, text_lens, max(text_lens))]

        control_values = args.pitch_control, args.energy_control, args.duration_control

        print(f"ğŸµ å¼€å§‹åˆæˆ...")
        synthesize(model, configs, vocoder, batchs, control_values)
        print(f"âœ… åˆæˆå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {train_config['path']['result_path']}")
        print(f"ğŸ§ éŸ³é¢‘æ–‡ä»¶: {args.speaker_id}_ipa_test.wav")
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 