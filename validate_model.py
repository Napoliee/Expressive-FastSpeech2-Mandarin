#!/usr/bin/env python3
"""
éªŒè¯æ¨¡å‹è®­ç»ƒæ•ˆæœçš„è„šæœ¬
åœ¨è®­ç»ƒé›†æ ·æœ¬ä¸Šæµ‹è¯•é‡å»ºèƒ½åŠ›
"""

import argparse
import os
import torch
import yaml
import numpy as np
from torch.utils.data import DataLoader

from utils.model import get_model, get_vocoder
from utils.tools import to_device, synth_one_sample
from dataset_ipa_fixed import Dataset
import soundfile as sf

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def validate_model_on_training_data():
    """åœ¨è®­ç»ƒæ•°æ®ä¸ŠéªŒè¯æ¨¡å‹æ•ˆæœ"""
    
    # Read Config
    preprocess_config = yaml.load(open("config/ESD-Chinese/preprocess.yaml", "r"), Loader=yaml.FullLoader)
    model_config = yaml.load(open("config/ESD-Chinese/model.yaml", "r"), Loader=yaml.FullLoader)
    train_config = yaml.load(open("config/ESD-Chinese/train.yaml", "r"), Loader=yaml.FullLoader)
    configs = (preprocess_config, model_config, train_config)

    # è·å–æœ€æ–°checkpoint
    checkpoint_dir = "output/ckpt/ESD-Chinese"
    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth.tar')]
    if not checkpoints:
        print("æœªæ‰¾åˆ°checkpointæ–‡ä»¶")
        return
    
    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('.')[0]))
    restore_step = int(latest_checkpoint.split('.')[0])
    
    print(f"ä½¿ç”¨checkpoint: {latest_checkpoint} (step {restore_step})")

    # åˆ›å»ºä¸€ä¸ªå‡çš„argså¯¹è±¡
    class Args:
        def __init__(self):
            self.restore_step = restore_step
    
    args = Args()

    # Get model
    model = get_model(args, configs, device, train=False)
    model.eval()

    # Load vocoder
    vocoder = get_vocoder(model_config, device)

    # åŠ è½½éªŒè¯æ•°æ®é›†ï¼ˆå°‘é‡æ ·æœ¬ï¼‰
    val_dataset = Dataset(
        "val_ipa.txt", 
        preprocess_config, 
        model_config, 
        train_config,
        sort=False,
        drop_last=False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=1,
        collate_fn=val_dataset.collate_fn,
    )

    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")
    
    # æµ‹è¯•å‰5ä¸ªæ ·æœ¬
    test_samples = []
    
    with torch.no_grad():
        for i, batchs in enumerate(val_loader):
            if i >= 5:  # åªæµ‹è¯•å‰5ä¸ªæ ·æœ¬
                break
            
            # val_loaderè¿”å›çš„æ˜¯batchåˆ—è¡¨ï¼Œéœ€è¦å–ç¬¬ä¸€ä¸ªbatch
            if not batchs or len(batchs) == 0:
                print(f"ç¬¬{i}ä¸ªbatchä¸ºç©ºï¼Œè·³è¿‡")
                continue
                
            batch = batchs[0]  # å–ç¬¬ä¸€ä¸ªbatch
            print(f"Batchç»“æ„é•¿åº¦: {len(batch)}")
            print(f"Batchå…ƒç´ ç±»å‹: {[type(x) for x in batch]}")
            
            batch = to_device(batch, device)
            
            # Forward pass
            try:
                output = model(*(batch[2:]))
            except Exception as e:
                print(f"æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
                print(f"Batch[2:]é•¿åº¦: {len(batch[2:])}")
                for j, item in enumerate(batch[2:]):
                    print(f"  Batch[{j+2}]ç±»å‹: {type(item)}, å½¢çŠ¶: {getattr(item, 'shape', 'N/A')}")
                continue
            
            # è·å–æ ·æœ¬ä¿¡æ¯
            basename = batch[0][0]
            speaker_id = batch[2][0].item()
            text_sequence = batch[6][0].cpu().numpy()
            raw_text = batch[1][0]
            
            print(f"\næ ·æœ¬ {i+1}: {basename}")
            print(f"åŸæ–‡: {raw_text}")
            print(f"è¯´è¯äºº: {speaker_id}")
            print(f"æ–‡æœ¬åºåˆ—é•¿åº¦: {len(text_sequence)}")
            
            # è·å–mel-spectrogramé¢„æµ‹å’ŒçœŸå®å€¼
            mel_prediction = output[1][0].detach().cpu().numpy()
            mel_target = batch[9][0].detach().cpu().numpy()
            
            print(f"é¢„æµ‹melå½¢çŠ¶: {mel_prediction.shape}")
            print(f"çœŸå®melå½¢çŠ¶: {mel_target.shape}")
            
            # è®¡ç®—é‡å»ºloss
            mel_loss = np.mean((mel_prediction - mel_target) ** 2)
            print(f"Melé‡å»ºMSE Loss: {mel_loss:.4f}")
            
            # ä½¿ç”¨vocoderç”ŸæˆéŸ³é¢‘
            if vocoder is not None:
                try:
                    # é‡å»ºéŸ³é¢‘ï¼ˆä½¿ç”¨çœŸå®melï¼‰
                    from utils.model import vocoder_infer
                    
                    mel_target_tensor = torch.FloatTensor(mel_target).unsqueeze(0).transpose(1, 2).to(device)
                    wav_reconstruction = vocoder_infer(
                        mel_target_tensor,
                        vocoder,
                        model_config,
                        preprocess_config,
                    )[0]
                    
                    # é¢„æµ‹éŸ³é¢‘
                    mel_prediction_tensor = torch.FloatTensor(mel_prediction).unsqueeze(0).transpose(1, 2).to(device)
                    wav_prediction = vocoder_infer(
                        mel_prediction_tensor,
                        vocoder,
                        model_config,
                        preprocess_config,
                    )[0]
                    
                    # ä¿å­˜éŸ³é¢‘
                    output_dir = "validation_outputs"
                    os.makedirs(output_dir, exist_ok=True)
                    
                    # ä¿å­˜é‡å»ºéŸ³é¢‘ï¼ˆGround Truth Mel -> Audioï¼‰
                    sf.write(
                        os.path.join(output_dir, f"{basename}_reconstruction.wav"),
                        wav_reconstruction,
                        22050
                    )
                    
                    # ä¿å­˜é¢„æµ‹éŸ³é¢‘ï¼ˆPredicted Mel -> Audioï¼‰
                    sf.write(
                        os.path.join(output_dir, f"{basename}_prediction.wav"),
                        wav_prediction,
                        22050
                    )
                    
                    print(f"âœ… éŸ³é¢‘å·²ä¿å­˜åˆ° {output_dir}/")
                    print(f"   - {basename}_reconstruction.wav (çœŸå®melé‡å»º)")
                    print(f"   - {basename}_prediction.wav (æ¨¡å‹é¢„æµ‹)")
                    
                    # æ£€æŸ¥éŸ³é¢‘è´¨é‡æŒ‡æ ‡
                    print(f"é‡å»ºéŸ³é¢‘é•¿åº¦: {len(wav_reconstruction)/22050:.2f}ç§’")
                    print(f"é¢„æµ‹éŸ³é¢‘é•¿åº¦: {len(wav_prediction)/22050:.2f}ç§’")
                    
                    # æ£€æŸ¥éŸ³é¢‘å¹…åº¦
                    print(f"é‡å»ºéŸ³é¢‘RMS: {np.sqrt(np.mean(wav_reconstruction**2)):.4f}")
                    print(f"é¢„æµ‹éŸ³é¢‘RMS: {np.sqrt(np.mean(wav_prediction**2)):.4f}")
                    
                except Exception as e:
                    print(f"âŒ Vocoderåˆæˆå¤±è´¥: {e}")
            
            test_samples.append({
                'basename': basename,
                'mel_loss': mel_loss,
                'mel_shape': mel_prediction.shape,
                'text_len': len(text_sequence)
            })
    
    # æ€»ç»“
    print(f"\n{'='*50}")
    print("éªŒè¯æ€»ç»“")
    print(f"{'='*50}")
    
    avg_mel_loss = np.mean([s['mel_loss'] for s in test_samples])
    print(f"å¹³å‡Melé‡å»ºLoss: {avg_mel_loss:.4f}")
    
    mel_shapes = [s['mel_shape'] for s in test_samples]
    print(f"Melé¢‘è°±å½¢çŠ¶èŒƒå›´: {mel_shapes}")
    
    text_lens = [s['text_len'] for s in test_samples]
    print(f"æ–‡æœ¬åºåˆ—é•¿åº¦èŒƒå›´: {min(text_lens)} - {max(text_lens)}")
    
    # åˆ¤æ–­è®­ç»ƒè´¨é‡
    if avg_mel_loss < 0.1:
        print("âœ… è®­ç»ƒè´¨é‡è‰¯å¥½ - Melé‡å»ºLossè¾ƒä½")
    elif avg_mel_loss < 0.5:
        print("âš ï¸  è®­ç»ƒè´¨é‡ä¸€èˆ¬ - Melé‡å»ºLossä¸­ç­‰")
    else:
        print("âŒ è®­ç»ƒè´¨é‡å·® - Melé‡å»ºLossè¿‡é«˜")
    
    print(f"\nğŸ“ éªŒè¯éŸ³é¢‘ä¿å­˜åœ¨: validation_outputs/")
    print("è¯·å¬è¿™äº›éŸ³é¢‘æ–‡ä»¶æ¥åˆ¤æ–­è®­ç»ƒæ•ˆæœ:")
    for sample in test_samples:
        print(f"  - {sample['basename']}_reconstruction.wav (åº”è¯¥æ¸…æ™°)")
        print(f"  - {sample['basename']}_prediction.wav (æ£€æŸ¥æ¨¡å‹é¢„æµ‹è´¨é‡)")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--samples", type=int, default=5, help="æµ‹è¯•æ ·æœ¬æ•°é‡")
    args = parser.parse_args()
    
    print("ğŸ” å¼€å§‹éªŒè¯æ¨¡å‹è®­ç»ƒæ•ˆæœ...")
    validate_model_on_training_data()

if __name__ == "__main__":
    main() 