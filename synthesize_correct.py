#!/usr/bin/env python3
"""
æ­£ç¡®çš„ä¸­æ–‡TTSæ¨ç†è„šæœ¬
å®Œå…¨éµå¾ªé¢„å¤„ç†æµç¨‹ï¼šMFAå¯¹é½ â†’ æå–IPAéŸ³ç´  â†’ IDè½¬æ¢
"""

import argparse
import os
import tempfile
import subprocess
import torch
import yaml
import numpy as np
import textgrid
import soundfile as sf
import json

from utils.model import get_model, get_vocoder
from utils.tools import to_device, synth_samples
from text.ipa_processor import text_to_sequence_ipa

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class CorrectChineseInference:
    def __init__(self, configs):
        self.preprocess_config, self.model_config, self.train_config = configs
        
    def create_temp_files_for_mfa(self, text, temp_dir):
        """ä¸ºMFAåˆ›å»ºä¸´æ—¶æ–‡ä»¶"""
        
        # åˆ›å»ºä¸´æ—¶éŸ³é¢‘æ–‡ä»¶ï¼ˆ1ç§’é™éŸ³ï¼‰
        wav_path = os.path.join(temp_dir, "temp.wav")
        silent_audio = np.zeros(22050, dtype=np.float32)
        sf.write(wav_path, silent_audio, 22050)
        
        # åˆ›å»ºæ–‡æœ¬æ–‡ä»¶
        lab_path = os.path.join(temp_dir, "temp.lab")
        with open(lab_path, 'w', encoding='utf-8') as f:
            f.write(text)
        
        return wav_path, lab_path

    def run_mfa_alignment(self, text, temp_dir):
        """è¿è¡ŒMFAå¯¹é½ï¼Œä¸é¢„å¤„ç†æ—¶å®Œå…¨ç›¸åŒçš„æµç¨‹"""
        
        print(f"ğŸ”¤ æ­£åœ¨å¯¹æ–‡æœ¬è¿›è¡ŒMFAå¯¹é½: {text}")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        wav_path, lab_path = self.create_temp_files_for_mfa(text, temp_dir)
        
        # è¾“å‡ºç›®å½•
        output_dir = os.path.join(temp_dir, "output")
        os.makedirs(output_dir, exist_ok=True)
        
        # ä½¿ç”¨ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„MFAå‘½ä»¤
        mfa_cmd = [
            "mfa", "align",
            temp_dir,
            "mandarin_mfa",  # ä¸­æ–‡è¯å…¸
            "mandarin_mfa",  # ä¸­æ–‡å£°å­¦æ¨¡å‹
            output_dir,
            "--clean"
        ]
        
        try:
            print("   è¿è¡ŒMFAå¯¹é½...")
            result = subprocess.run(mfa_cmd, capture_output=True, text=True, timeout=120)
            
            if result.returncode != 0:
                print(f"âŒ MFAå¯¹é½å¤±è´¥:")
                print(f"   stderr: {result.stderr}")
                return None
            
            # æ£€æŸ¥ç”Ÿæˆçš„TextGridæ–‡ä»¶
            textgrid_path = os.path.join(output_dir, "temp.TextGrid")
            if os.path.exists(textgrid_path):
                return self.extract_phonemes_from_textgrid(textgrid_path)
            else:
                print("âŒ æœªç”ŸæˆTextGridæ–‡ä»¶")
                return None
                
        except subprocess.TimeoutExpired:
            print("âŒ MFAå¯¹é½è¶…æ—¶")
            return None
        except Exception as e:
            print(f"âŒ MFAå¯¹é½å¼‚å¸¸: {e}")
            return None

    def extract_phonemes_from_textgrid(self, textgrid_path):
        """ä»TextGridæå–IPAéŸ³ç´ ï¼Œä¸é¢„å¤„ç†å™¨å®Œå…¨ç›¸åŒçš„é€»è¾‘"""
        
        try:
            tg = textgrid.TextGrid.fromFile(textgrid_path)
            
            # æŸ¥æ‰¾phoneså±‚
            phone_tier = None
            for tier in tg.tiers:
                if tier.name.lower() in ['phones', 'phone']:
                    phone_tier = tier
                    break
            
            if phone_tier is None:
                print("âŒ TextGridä¸­æœªæ‰¾åˆ°phoneså±‚")
                return None
            
            # æå–éŸ³ç´ ï¼Œä½¿ç”¨ä¸preprocessor.pyç›¸åŒçš„é€»è¾‘
            sil_phones = ["sil", "sp", "spn"]
            phones = []
            
            for interval in phone_tier:
                phone = interval.mark.strip()
                if phone and phone != '':
                    phones.append(phone)
            
            # è¿‡æ»¤å¼€å¤´å’Œç»“å°¾çš„é™éŸ³ï¼ˆä¸preprocessoré€»è¾‘ä¸€è‡´ï¼‰
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªéé™éŸ³éŸ³ç´ 
            start_idx = 0
            for i, p in enumerate(phones):
                if p not in sil_phones:
                    start_idx = i
                    break
            
            # æ‰¾åˆ°æœ€åä¸€ä¸ªéé™éŸ³éŸ³ç´ 
            end_idx = len(phones)
            for i in range(len(phones)-1, -1, -1):
                if phones[i] not in sil_phones:
                    end_idx = i + 1
                    break
            
            # æå–æœ‰æ•ˆéŸ³ç´ 
            valid_phones = phones[start_idx:end_idx]
            
            print(f"âœ… æå–åˆ°éŸ³ç´ : {valid_phones}")
            print(f"   éŸ³ç´ æ•°é‡: {len(valid_phones)}")
            
            return valid_phones
            
        except Exception as e:
            print(f"âŒ æå–éŸ³ç´ å¤±è´¥: {e}")
            return None

    def phonemes_to_id_sequence(self, phonemes):
        """å°†IPAéŸ³ç´ è½¬æ¢ä¸ºIDåºåˆ—ï¼Œä½¿ç”¨è®­ç»ƒæ—¶çš„ç›¸åŒæ–¹æ³•"""
        
        # æ„é€ IPAæ ¼å¼å­—ç¬¦ä¸²
        ipa_text = "{" + " ".join(phonemes) + "}"
        print(f"ğŸ”¢ IPAéŸ³ç´ å­—ç¬¦ä¸²: {ipa_text}")
        
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„è½¬æ¢æ–¹æ³•
        try:
            sequence = text_to_sequence_ipa(ipa_text)
            print(f"âœ… IDåºåˆ—: {sequence}")
            print(f"   åºåˆ—é•¿åº¦: {len(sequence)}")
            return np.array(sequence, dtype=np.int64)
        except Exception as e:
            print(f"âŒ éŸ³ç´ è½¬IDå¤±è´¥: {e}")
            return None

    def synthesize_with_correct_flow(self, text, speaker_id, emotion_id):
        """ä½¿ç”¨æ­£ç¡®æµç¨‹è¿›è¡Œåˆæˆ"""
        
        print(f"ğŸ¤ æ­£ç¡®æµç¨‹æ¨ç†")
        print(f"æ–‡æœ¬: {text}")
        print(f"è¯´è¯äºº: {speaker_id}")
        print(f"æƒ…æ„Ÿ: {emotion_id}")
        print("=" * 50)
        
        # æ­¥éª¤1ï¼šMFAå¯¹é½
        with tempfile.TemporaryDirectory() as temp_dir:
            phonemes = self.run_mfa_alignment(text, temp_dir)
            
            if phonemes is None:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä»è®­ç»ƒæ•°æ®ä¸­æŸ¥æ‰¾ç›¸åŒæ–‡æœ¬
                print("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼šä»è®­ç»ƒæ•°æ®æŸ¥æ‰¾...")
                phonemes = self.find_from_training_data(text)
                
                if phonemes is None:
                    print("âŒ æ— æ³•è·å–éŸ³ç´ åºåˆ—")
                    return None
        
        # æ­¥éª¤2ï¼šè½¬æ¢ä¸ºIDåºåˆ—
        id_sequence = self.phonemes_to_id_sequence(phonemes)
        if id_sequence is None:
            return None
        
        # æ­¥éª¤3ï¼šå‡†å¤‡æ¨¡å‹è¾“å…¥
        return self.prepare_model_input(text, speaker_id, emotion_id, id_sequence)

    def find_from_training_data(self, target_text):
        """ä»è®­ç»ƒæ•°æ®ä¸­æŸ¥æ‰¾ç›¸åŒæ–‡æœ¬çš„éŸ³ç´ ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        
        data_files = [
            os.path.join(self.preprocess_config["path"]["preprocessed_path"], "train_ipa.txt"),
            os.path.join(self.preprocess_config["path"]["preprocessed_path"], "val_ipa.txt")
        ]
        
        for data_file in data_files:
            if not os.path.exists(data_file):
                continue
                
            with open(data_file, "r", encoding="utf-8") as f:
                for line in f:
                    parts = line.strip().split("|")
                    if len(parts) >= 4:
                        phoneme_text = parts[2]  # IPAéŸ³ç´ å­—æ®µ
                        raw_text = parts[3]     # åŸæ–‡
                        
                        if raw_text.strip() == target_text.strip():
                            print(f"âœ… åœ¨è®­ç»ƒæ•°æ®ä¸­æ‰¾åˆ°åŒ¹é…æ–‡æœ¬")
                            # æå–éŸ³ç´ 
                            if phoneme_text.startswith('{') and phoneme_text.endswith('}'):
                                phonemes = phoneme_text[1:-1].split()
                                print(f"   éŸ³ç´ : {phonemes}")
                                return phonemes
        
        print(f"âš ï¸  è®­ç»ƒæ•°æ®ä¸­æ— æ­¤æ–‡æœ¬: {target_text}")
        return None

    def prepare_model_input(self, text, speaker_id, emotion_id, id_sequence):
        """å‡†å¤‡æ¨¡å‹è¾“å…¥"""
        
        # åŠ è½½æ˜ å°„
        with open(os.path.join(self.preprocess_config["path"]["preprocessed_path"], "speakers.json")) as f:
            speaker_map = json.load(f)
        
        with open(os.path.join(self.preprocess_config["path"]["preprocessed_path"], "emotions.json")) as f:
            emotion_data = json.load(f)
            emotion_map = emotion_data["emotion_dict"]
            arousal_map = emotion_data["arousal_dict"]
            valence_map = emotion_data["valence_dict"]
        
        # å‡†å¤‡æ•°æ®
        ids = [f"{speaker_id}_correct"]
        raw_texts = [text]
        speakers = np.array([speaker_map[speaker_id]], dtype=np.int64)
        emotions = np.array([emotion_map[emotion_id]], dtype=np.int64)
        arousals = np.array([arousal_map[emotion_id]], dtype=np.float32)
        valences = np.array([valence_map[emotion_id]], dtype=np.float32)
        
        text_lens = np.array([len(id_sequence)], dtype=np.int64)
        
        # Padding
        from utils.tools import pad_1D
        texts = pad_1D([id_sequence])
        
        batch = (ids, raw_texts, speakers, emotions, arousals, valences, texts, text_lens, max(text_lens))
        
        return batch

def synthesize(model, configs, vocoder, batchs, control_values):
    """åˆæˆå‡½æ•°"""
    preprocess_config, model_config, train_config = configs
    pitch_control, energy_control, duration_control = control_values

    for batch in batchs:
        batch = to_device(batch, device)
        with torch.no_grad():
            output = model(
                *(batch[2:]),
                p_control=pitch_control,
                e_control=energy_control,
                d_control=duration_control
            )
            synth_samples(
                batch,
                output,
                vocoder,
                model_config,
                preprocess_config,
                train_config["path"]["result_path"],
            )

def get_latest_checkpoint():
    """è·å–æœ€æ–°checkpoint"""
    checkpoint_dir = "output/ckpt/ESD-Chinese"
    if not os.path.exists(checkpoint_dir):
        return None
    
    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth.tar')]
    if not checkpoints:
        return None
    
    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('.')[0]))
    return int(latest_checkpoint.split('.')[0])

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--restore_step", type=int, default=None)
    parser.add_argument("--text", type=str, required=True, help="è¦åˆæˆçš„ä¸­æ–‡æ–‡æœ¬")
    parser.add_argument("--speaker_id", type=str, default="0001", help="è¯´è¯äººID")
    parser.add_argument("--emotion_id", type=str, default="ä¸­ç«‹", help="æƒ…æ„ŸID")
    parser.add_argument("--pitch_control", type=float, default=1.0)
    parser.add_argument("--energy_control", type=float, default=1.0)
    parser.add_argument("--duration_control", type=float, default=1.0)
    args = parser.parse_args()

    # è·å–checkpoint
    if args.restore_step is None:
        args.restore_step = get_latest_checkpoint()
        if args.restore_step is None:
            print("âŒ æœªæ‰¾åˆ°checkpointæ–‡ä»¶")
            return

    # è¯»å–é…ç½®
    preprocess_config = yaml.load(open("config/ESD-Chinese/preprocess.yaml", "r"), Loader=yaml.FullLoader)
    model_config = yaml.load(open("config/ESD-Chinese/model.yaml", "r"), Loader=yaml.FullLoader)
    train_config = yaml.load(open("config/ESD-Chinese/train.yaml", "r"), Loader=yaml.FullLoader)
    configs = (preprocess_config, model_config, train_config)

    # åŠ è½½æ¨¡å‹
    model = get_model(args, configs, device, train=False)
    vocoder = get_vocoder(model_config, device)

    # åˆ›å»ºæ¨ç†å™¨
    inferencer = CorrectChineseInference(configs)
    
    # æ‰§è¡Œæ­£ç¡®æµç¨‹çš„æ¨ç†
    batch = inferencer.synthesize_with_correct_flow(args.text, args.speaker_id, args.emotion_id)
    
    if batch is not None:
        control_values = args.pitch_control, args.energy_control, args.duration_control
        
        print(f"\nğŸµ å¼€å§‹åˆæˆ...")
        synthesize(model, configs, vocoder, [batch], control_values)
        print(f"âœ… åˆæˆå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {train_config['path']['result_path']}")
        print(f"ğŸ§ éŸ³é¢‘æ–‡ä»¶: {args.speaker_id}_correct.wav")
    else:
        print("âŒ æ¨ç†å¤±è´¥")

if __name__ == "__main__":
    main() 