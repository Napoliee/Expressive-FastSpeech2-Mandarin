#!/usr/bin/env python3
"""
ä»è®­ç»ƒæ•°æ®ä¸­æŸ¥æ‰¾ç›¸åŒæ–‡æœ¬çš„éŸ³ç´ åºåˆ—è¿›è¡Œæ¨ç†
ç¡®ä¿ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
"""

import argparse
import os
import torch
import yaml
import numpy as np
from torch.utils.data import DataLoader

from utils.model import get_model, get_vocoder
from utils.tools import to_device, synth_samples
from text.ipa_processor import text_to_sequence_ipa

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def find_phonemes_from_training_data(target_text):
    """ä»è®­ç»ƒæ•°æ®ä¸­æŸ¥æ‰¾ç›¸åŒæ–‡æœ¬çš„éŸ³ç´ åºåˆ—"""
    
    print(f"åœ¨è®­ç»ƒæ•°æ®ä¸­æŸ¥æ‰¾æ–‡æœ¬: {target_text}")
    
    # æœç´¢è®­ç»ƒå’ŒéªŒè¯æ•°æ®
    data_files = [
        "preprocessed_data/ESD-Chinese/train_ipa.txt",
        "preprocessed_data/ESD-Chinese/val_ipa.txt"
    ]
    
    found_phonemes = []
    
    for data_file in data_files:
        if not os.path.exists(data_file):
            continue
            
        with open(data_file, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f):
                parts = line.strip().split("|")
                if len(parts) >= 4:
                    basename = parts[0]
                    speaker = parts[1]
                    phonemes = parts[2]  # IPAéŸ³ç´ å­—ç¬¦ä¸² "{...}"
                    raw_text = parts[3]
                    
                    if raw_text.strip() == target_text.strip():
                        print(f"æ‰¾åˆ°åŒ¹é…æ–‡æœ¬åœ¨ {data_file}:{line_num+1}")
                        print(f"  æ–‡ä»¶: {basename}")
                        print(f"  è¯´è¯äºº: {speaker}")
                        print(f"  åŸæ–‡: {raw_text}")
                        print(f"  éŸ³ç´ : {phonemes}")
                        
                        found_phonemes.append({
                            'basename': basename,
                            'speaker': speaker,
                            'phonemes': phonemes,
                            'raw_text': raw_text,
                            'source_file': data_file
                        })
    
    if found_phonemes:
        print(f"æ€»å…±æ‰¾åˆ° {len(found_phonemes)} ä¸ªåŒ¹é…é¡¹")
        return found_phonemes
    else:
        print("æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡æœ¬")
        return None

def list_available_texts():
    """åˆ—å‡ºè®­ç»ƒæ•°æ®ä¸­å¯ç”¨çš„æ–‡æœ¬æ ·æœ¬"""
    
    print("=== è®­ç»ƒæ•°æ®ä¸­çš„å¯ç”¨æ–‡æœ¬æ ·æœ¬ ===")
    
    data_files = [
        "preprocessed_data/ESD-Chinese/train_ipa.txt",
        "preprocessed_data/ESD-Chinese/val_ipa.txt"
    ]
    
    all_texts = set()
    
    for data_file in data_files:
        if not os.path.exists(data_file):
            continue
            
        with open(data_file, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split("|")
                if len(parts) >= 4:
                    raw_text = parts[3].strip()
                    all_texts.add(raw_text)
    
    # æ˜¾ç¤ºå‰20ä¸ªæ–‡æœ¬æ ·æœ¬
    texts_list = sorted(list(all_texts))
    print("å‰20ä¸ªå¯ç”¨æ–‡æœ¬:")
    for i, text in enumerate(texts_list[:20]):
        print(f"  {i+1}: {text}")
    
    print(f"\næ€»å…± {len(texts_list)} ä¸ªä¸åŒçš„æ–‡æœ¬")
    print("\nä½ å¯ä»¥ä½¿ç”¨è¿™äº›æ–‡æœ¬è¿›è¡Œæ¨ç†ï¼Œç¡®ä¿éŸ³ç´ åŒ¹é…è®­ç»ƒæ•°æ®")
    
    return texts_list

def get_latest_checkpoint():
    """è·å–æœ€æ–°çš„checkpoint"""
    checkpoint_dir = "output/ckpt/ESD-Chinese"
    if not os.path.exists(checkpoint_dir):
        print(f"Checkpointç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")
        return None
    
    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth.tar')]
    if not checkpoints:
        print(f"åœ¨{checkpoint_dir}ä¸­æœªæ‰¾åˆ°.pth.taræ–‡ä»¶")
        return None
    
    print(f"æ‰¾åˆ°checkpointæ–‡ä»¶: {checkpoints}")
    
    # æå–æ­¥æ•°å¹¶æ’åº
    checkpoint_steps = []
    for cp in checkpoints:
        try:
            # æ–‡ä»¶åæ ¼å¼åº”è¯¥æ˜¯ "100000.pth.tar"
            step = int(cp.split('.')[0])
            checkpoint_steps.append(step)
        except ValueError:
            print(f"æ— æ³•ä»æ–‡ä»¶å{cp}ä¸­æå–æ­¥æ•°")
            continue
    
    if checkpoint_steps:
        max_step = max(checkpoint_steps)
        print(f"æœ€æ–°checkpointæ­¥æ•°: {max_step}")
        return max_step
    return None

def synthesize(model, configs, vocoder, batchs, control_values):
    preprocess_config, model_config, train_config = configs
    pitch_control, energy_control, duration_control = control_values

    for batch in batchs:
        batch = to_device(batch, device)
        with torch.no_grad():
            # Forward
            output = model(
                *(batch[2:]),
                p_control=pitch_control,
                e_control=energy_control,
                d_control=duration_control
            )
            synth_samples(
                batch,
                output,
                vocoder,
                model_config,
                preprocess_config,
                train_config["path"]["result_path"],
            )

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--restore_step", type=int, default=None, help="ä½¿ç”¨ç‰¹å®šæ­¥æ•°çš„checkpointï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°çš„")
    parser.add_argument("--text", type=str, default=None, help="è¦åˆæˆçš„ä¸­æ–‡æ–‡æœ¬ï¼ˆå¿…é¡»åœ¨è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨ï¼‰")
    parser.add_argument("--speaker_id", type=str, default=None, help="è¯´è¯äººIDï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨åŸå§‹è¯´è¯äººï¼‰")
    parser.add_argument("--emotion", type=str, default=None, help="æƒ…æ„Ÿï¼ˆå¯é€‰ï¼‰: å¼€å¿ƒ/ä¼¤å¿ƒ/æƒŠè®¶/æ„¤æ€’/ä¸­ç«‹")
    parser.add_argument("--list_texts", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ–‡æœ¬")
    parser.add_argument("--pitch_control", type=float, default=1.0)
    parser.add_argument("--energy_control", type=float, default=1.0)
    parser.add_argument("--duration_control", type=float, default=1.0)
    args = parser.parse_args()

    # å¦‚æœç”¨æˆ·è¦æ±‚åˆ—å‡ºå¯ç”¨æ–‡æœ¬
    if args.list_texts:
        list_available_texts()
        return

    if args.text is None:
        print("è¯·æä¾›è¦åˆæˆçš„æ–‡æœ¬ï¼Œæˆ–ä½¿ç”¨ --list_texts æŸ¥çœ‹å¯ç”¨æ–‡æœ¬")
        return

    # è·å–æœ€æ–°checkpoint
    if args.restore_step is None:
        latest_step = get_latest_checkpoint()
        if latest_step is None:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•checkpointæ–‡ä»¶")
            return
        args.restore_step = latest_step
        print(f"ğŸ”„ ä½¿ç”¨æœ€æ–°checkpoint: {args.restore_step}")

    # ä»è®­ç»ƒæ•°æ®ä¸­æŸ¥æ‰¾åŒ¹é…çš„éŸ³ç´ 
    found_items = find_phonemes_from_training_data(args.text)
    
    if not found_items:
        print("âŒ æœªåœ¨è®­ç»ƒæ•°æ®ä¸­æ‰¾åˆ°æ­¤æ–‡æœ¬")
        print("è¯·ä½¿ç”¨ --list_texts æŸ¥çœ‹å¯ç”¨çš„æ–‡æœ¬")
        return
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹
    selected_item = found_items[0]
    phonemes_str = selected_item['phonemes']
    original_speaker = selected_item['speaker']
    
    # ä½¿ç”¨æŒ‡å®šçš„è¯´è¯äººIDï¼Œæˆ–è€…ä½¿ç”¨åŸå§‹è¯´è¯äºº
    target_speaker = args.speaker_id if args.speaker_id else original_speaker
    
    print(f"ä½¿ç”¨éŸ³ç´ åºåˆ—: {phonemes_str}")
    print(f"åŸå§‹è¯´è¯äºº: {original_speaker}")
    print(f"ç›®æ ‡è¯´è¯äºº: {target_speaker}")
    
    # Read Config
    preprocess_config = yaml.load(open("config/ESD-Chinese/preprocess.yaml", "r"), Loader=yaml.FullLoader)
    model_config = yaml.load(open("config/ESD-Chinese/model.yaml", "r"), Loader=yaml.FullLoader)
    train_config = yaml.load(open("config/ESD-Chinese/train.yaml", "r"), Loader=yaml.FullLoader)
    configs = (preprocess_config, model_config, train_config)

    print(f"ğŸ¤ ä½¿ç”¨è®­ç»ƒæ•°æ®éŸ³ç´ è¿›è¡Œæ¨ç†")
    print(f"æ£€æŸ¥ç‚¹: {args.restore_step}")
    print(f"è¾“å…¥æ–‡æœ¬: {args.text}")
    print(f"è¯´è¯äºº: {target_speaker}")
    print(f"æƒ…æ„Ÿ: {args.emotion}")

    # Get model
    model = get_model(args, configs, device, train=False)

    # Load vocoder
    vocoder = get_vocoder(model_config, device)

    # å‡†å¤‡è¾“å…¥æ•°æ®
    ids = [f"{target_speaker}_from_training"]
    raw_texts = [args.text]
    speakers = np.array([int(target_speaker)], dtype=np.int64)
    
    # æƒ…æ„Ÿæ˜ å°„
    emotion_map = {"å¼€å¿ƒ": 0, "ä¼¤å¿ƒ": 1, "æƒŠè®¶": 2, "æ„¤æ€’": 3, "ä¸­ç«‹": 4}
    if args.emotion:
        emotion_id = emotion_map.get(args.emotion, 4)
    else:
        emotion_id = 4  # é»˜è®¤ä¸­ç«‹
        
    emotions = np.array([emotion_id], dtype=np.int64)
    arousals = np.array([0.5], dtype=np.float32)
    valences = np.array([0.5], dtype=np.float32)
    
    # è½¬æ¢ä¸ºåºåˆ—
    try:
        text_sequence = np.array(text_to_sequence_ipa(phonemes_str), dtype=np.int64)
        text_lens = np.array([len(text_sequence)], dtype=np.int64)
        
        print(f"éŸ³ç´ åºåˆ—é•¿åº¦: {text_lens[0]}")
        print(f"éŸ³ç´ IDåºåˆ—: {text_sequence}")
        
        # å¯¹textsè¿›è¡Œpaddingå¤„ç†ï¼ˆæ‰‹åŠ¨paddingï¼Œå› ä¸ºæˆ‘ä»¬åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼‰
        from utils.tools import pad_1D
        texts = pad_1D([text_sequence])
        
        batchs = [(ids, raw_texts, speakers, emotions, arousals, valences, texts, text_lens, max(text_lens))]

        control_values = args.pitch_control, args.energy_control, args.duration_control

        print(f"ğŸµ å¼€å§‹åˆæˆ...")
        synthesize(model, configs, vocoder, batchs, control_values)
        print(f"âœ… åˆæˆå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {train_config['path']['result_path']}")
        print(f"ğŸ§ éŸ³é¢‘æ–‡ä»¶: {target_speaker}_from_training.wav")
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 