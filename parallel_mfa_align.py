#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¹¶è¡ŒMFAæ‹¼éŸ³å¯¹é½ - åå°è¿è¡Œç‰ˆæœ¬
"""

import os
import shutil
import subprocess
import time
import json
from pathlib import Path
import concurrent.futures
from datetime import datetime

def create_small_batches(source_dir, batch_size=200):
    """å°†æ•°æ®åˆ†æˆå°æ‰¹æ¬¡ï¼ˆ200ä¸ªæ–‡ä»¶ï¼‰"""
    
    source_path = Path(source_dir)
    batches_dir = Path("./raw_data/ESD-Chinese-Pinyin-SmallBatches")
    
    # æ¸…ç†æ—§çš„æ‰¹æ¬¡ç›®å½•
    if batches_dir.exists():
        shutil.rmtree(batches_dir)
    
    batches_dir.mkdir(parents=True, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
    all_files = []
    for speaker_dir in source_path.iterdir():
        if speaker_dir.is_dir():
            speaker_files = list(speaker_dir.glob("*.wav"))
            for wav_file in speaker_files:
                lab_file = wav_file.with_suffix(".lab")
                if lab_file.exists():
                    all_files.append((wav_file, lab_file, speaker_dir.name))
    
    print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(all_files)} å¯¹éŸ³é¢‘+æ ‡æ³¨æ–‡ä»¶")
    
    # åˆ†æ‰¹å¤„ç†
    batches = []
    for i in range(0, len(all_files), batch_size):
        batch_num = i // batch_size + 1
        batch_files = all_files[i:i + batch_size]
        
        batch_dir = batches_dir / f"batch_{batch_num:03d}"
        batch_dir.mkdir(exist_ok=True)
        
        # æŒ‰è¯´è¯äººç»„ç»‡æ–‡ä»¶
        speakers_in_batch = {}
        for wav_file, lab_file, speaker_id in batch_files:
            if speaker_id not in speakers_in_batch:
                speakers_in_batch[speaker_id] = []
                speaker_batch_dir = batch_dir / speaker_id
                speaker_batch_dir.mkdir(exist_ok=True)
            
            # å¤åˆ¶æ–‡ä»¶
            dst_wav = batch_dir / speaker_id / wav_file.name
            dst_lab = batch_dir / speaker_id / lab_file.name
            
            shutil.copy2(wav_file, dst_wav)
            shutil.copy2(lab_file, dst_lab)
            
            speakers_in_batch[speaker_id].append(wav_file.name)
        
        batches.append({
            'batch_num': batch_num,
            'batch_dir': batch_dir,
            'file_count': len(batch_files),
            'speakers': list(speakers_in_batch.keys())
        })
        
        print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_num}: {len(batch_files)} ä¸ªæ–‡ä»¶, {len(speakers_in_batch)} ä¸ªè¯´è¯äºº")
    
    return batches

def run_single_batch_align(batch_info):
    """å¯¹å•ä¸ªæ‰¹æ¬¡è¿è¡ŒMFAå¯¹é½"""
    
    batch_dir = batch_info['batch_dir']
    batch_num = batch_info['batch_num']
    
    # è¾“å‡ºç›®å½•
    output_dir = Path(f"./preprocessed_data/ESD-Chinese-Pinyin/TextGrid_batch_{batch_num:03d}")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ—¥å¿—æ–‡ä»¶
    log_file = Path(f"./logs/mfa_batch_{batch_num:03d}.log")
    log_file.parent.mkdir(exist_ok=True)
    
    start_time = time.time()
    
    print(f"ğŸš€ [{datetime.now().strftime('%H:%M:%S')}] å¼€å§‹æ‰¹æ¬¡ {batch_num}")
    
    # æ„å»ºMFAå‘½ä»¤
    cmd = [
        "conda", "run", "-n", "aligner",
        "mfa", "align",
        "--clean",  # æ¸…ç†ä¸­é—´æ–‡ä»¶
        "--num_jobs", "2",  # é™åˆ¶å¹¶è¡Œåº¦
        str(batch_dir),
        "mandarin_pinyin",
        "mandarin_mfa", 
        str(output_dir)
    ]
    
    try:
        # è¿è¡ŒMFAå¯¹é½ï¼Œå°†è¾“å‡ºé‡å®šå‘åˆ°æ—¥å¿—æ–‡ä»¶
        with open(log_file, 'w', encoding='utf-8') as f:
            result = subprocess.run(cmd, stdout=f, stderr=subprocess.STDOUT, timeout=7200)  # 2å°æ—¶è¶…æ—¶
        
        end_time = time.time()
        duration = end_time - start_time
        
        if result.returncode == 0:
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
            textgrid_files = list(output_dir.rglob("*.TextGrid"))
            print(f"âœ… [{datetime.now().strftime('%H:%M:%S')}] æ‰¹æ¬¡ {batch_num} å®Œæˆ - {len(textgrid_files)} æ–‡ä»¶ - {duration/60:.1f}åˆ†é’Ÿ")
            return {
                'batch_num': batch_num,
                'success': True,
                'duration': duration,
                'output_files': len(textgrid_files),
                'file_count': batch_info['file_count']
            }
        else:
            print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] æ‰¹æ¬¡ {batch_num} å¤±è´¥")
            return {
                'batch_num': batch_num,
                'success': False,
                'duration': duration,
                'output_files': 0,
                'file_count': batch_info['file_count']
            }
            
    except subprocess.TimeoutExpired:
        print(f"â±ï¸ [{datetime.now().strftime('%H:%M:%S')}] æ‰¹æ¬¡ {batch_num} è¶…æ—¶")
        return {
            'batch_num': batch_num,
            'success': False,
            'duration': 7200,
            'output_files': 0,
            'file_count': batch_info['file_count']
        }
    except Exception as e:
        print(f"ğŸ’¥ [{datetime.now().strftime('%H:%M:%S')}] æ‰¹æ¬¡ {batch_num} å¼‚å¸¸: {e}")
        return {
            'batch_num': batch_num,
            'success': False,
            'duration': 0,
            'output_files': 0,
            'file_count': batch_info['file_count']
        }

def save_progress(results, progress_file="./logs/mfa_progress.json"):
    """ä¿å­˜è¿›åº¦"""
    Path(progress_file).parent.mkdir(exist_ok=True)
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

def main():
    """ä¸»å‡½æ•° - å¹¶è¡Œå¤„ç†"""
    
    source_dir = "./raw_data/ESD-Chinese-Pinyin"
    batch_size = 200  # æ¯æ‰¹200ä¸ªæ–‡ä»¶
    max_workers = 3   # æœ€å¤š3ä¸ªå¹¶è¡Œè¿›ç¨‹
    
    print("ğŸ¯ å¹¶è¡ŒMFAæ‹¼éŸ³å¯¹é½")
    print(f"ğŸ“‚ æºç›®å½•: {source_dir}")
    print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"ğŸ”§ å¹¶è¡Œè¿›ç¨‹: {max_workers}")
    
    # 1. åˆ›å»ºæ‰¹æ¬¡ç›®å½•
    print(f"\nğŸ“¦ åˆ›å»ºæ‰¹æ¬¡ç›®å½•...")
    batches = create_small_batches(source_dir, batch_size)
    
    if not batches:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
        return
    
    print(f"\nğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {len(batches)} ä¸ªæ‰¹æ¬¡")
    print(f"â±ï¸  é¢„è®¡è€—æ—¶: {len(batches) * 10 / max_workers:.0f} - {len(batches) * 30 / max_workers:.0f} åˆ†é’Ÿ")
    
    # 2. å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
    all_results = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_batch = {executor.submit(run_single_batch_align, batch): batch for batch in batches}
        
        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_batch):
            result = future.result()
            all_results.append(result)
            
            # ä¿å­˜è¿›åº¦
            save_progress(all_results)
            
            # æ‰“å°å½“å‰è¿›åº¦
            completed = len(all_results)
            total = len(batches)
            successful = sum(1 for r in all_results if r['success'])
            
            print(f"\nğŸ“Š è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%) - æˆåŠŸ: {successful}")
    
    # 3. åˆå¹¶ç»“æœ
    successful_batches = [r for r in all_results if r['success']]
    failed_batches = [r for r in all_results if not r['success']]
    
    if successful_batches:
        print(f"\nğŸ”— åˆå¹¶TextGridæ–‡ä»¶...")
        merge_textgrids_from_results(successful_batches)
    
    # 4. æœ€ç»ˆç»Ÿè®¡
    total_files = sum(r['file_count'] for r in all_results)
    successful_files = sum(r['output_files'] for r in successful_batches)
    total_time = sum(r['duration'] for r in all_results)
    
    print(f"\nğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   æ€»æ‰¹æ¬¡: {len(batches)}")
    print(f"   æˆåŠŸæ‰¹æ¬¡: {len(successful_batches)}")
    print(f"   å¤±è´¥æ‰¹æ¬¡: {len(failed_batches)}")
    print(f"   æ€»æ–‡ä»¶: {total_files}")
    print(f"   æˆåŠŸæ–‡ä»¶: {successful_files}")
    print(f"   æˆåŠŸç‡: {successful_files/total_files*100:.1f}%")
    print(f"   æ€»è€—æ—¶: {total_time/3600:.1f} å°æ—¶")
    
    if failed_batches:
        print(f"\nâŒ å¤±è´¥æ‰¹æ¬¡: {[r['batch_num'] for r in failed_batches]}")
    
    print(f"\nğŸ“ TextGridæ–‡ä»¶ä½ç½®: ./preprocessed_data/ESD-Chinese-Pinyin/TextGrid")

def merge_textgrids_from_results(successful_results):
    """ä»ç»“æœä¸­åˆå¹¶TextGridæ–‡ä»¶"""
    
    final_output_dir = Path("./preprocessed_data/ESD-Chinese-Pinyin/TextGrid")
    final_output_dir.mkdir(parents=True, exist_ok=True)
    
    total_merged = 0
    
    for result in successful_results:
        batch_num = result['batch_num']
        batch_output_dir = Path(f"./preprocessed_data/ESD-Chinese-Pinyin/TextGrid_batch_{batch_num:03d}")
        
        if not batch_output_dir.exists():
            continue
        
        # å¤åˆ¶æ‰€æœ‰TextGridæ–‡ä»¶
        for textgrid_file in batch_output_dir.rglob("*.TextGrid"):
            # ä¿æŒè¯´è¯äººç›®å½•ç»“æ„
            relative_path = textgrid_file.relative_to(batch_output_dir)
            dst_file = final_output_dir / relative_path
            
            # åˆ›å»ºç›®æ ‡ç›®å½•
            dst_file.parent.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(textgrid_file, dst_file)
            total_merged += 1
    
    print(f"âœ… æ€»å…±åˆå¹¶äº† {total_merged} ä¸ªTextGridæ–‡ä»¶")
    return total_merged

if __name__ == "__main__":
    main() 